{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gyhai/CurricuLLM\n"
     ]
    }
   ],
   "source": [
    "%cd /home/gyhai/CurricuLLM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gpt.utils import file_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_curriculum_training_log(logger_path):\n",
    "    training_log = np.load(logger_path + \"/evaluations.npz\", allow_pickle=True)\n",
    "\n",
    "    reward_dict = training_log[\"results_dict\"]\n",
    "    success = training_log[\"successes\"].mean(axis=1)\n",
    "    # successes (num_eval_rounds, num_envs)\n",
    "    # reward_dict (num_eval_rounds, num_envs, dict of rewards)\n",
    "\n",
    "    averaged_dicts = []\n",
    "\n",
    "    for row in reward_dict:\n",
    "        sum_dict = {}\n",
    "        for col in row:\n",
    "            for key in col:\n",
    "                sum_dict[key] = sum_dict.get(key, 0) + col[key]\n",
    "\n",
    "        avg_dict = {key: value/len(row) for key, value in sum_dict.items()}\n",
    "        averaged_dicts.append(avg_dict)\n",
    "\n",
    "    reward_df = pd.DataFrame(averaged_dicts)\n",
    "\n",
    "    return reward_df, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_log(logger_path):\n",
    "    training_log = np.load(logger_path + \"/evaluations.npz\", allow_pickle=True)\n",
    "    \n",
    "    try:\n",
    "        reward_main = training_log[\"results\"].mean(axis=1)\n",
    "    except:\n",
    "        reward_main = None\n",
    "    try:\n",
    "        success = training_log[\"successes\"].mean(axis=1)\n",
    "    except:\n",
    "        success = None\n",
    "    \n",
    "    return reward_main, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_curriculum(logger_path):\n",
    "    # extract curriculum and return list of dictionaries with task details\n",
    "    curriculum_txt = file_to_string(logger_path + \"curriculum.md\")\n",
    "    # Split the string into individual task sections\n",
    "    task_sections = re.split(r'\\n\\n(?=Task)', curriculum_txt)\n",
    "\n",
    "    # Function to extract details from each task section\n",
    "    def extract_task_details(task_section):\n",
    "\n",
    "        details = {}\n",
    "        lines = task_section.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('Task'):\n",
    "                details['Task'] = line.split(' ')[1]\n",
    "            elif line.startswith('Name:'):\n",
    "                details['Name'] = line.split(': ')[1]\n",
    "            elif line.startswith('Description:'):\n",
    "                details['Description'] = line.split(': ')[1]\n",
    "            elif line.startswith('Reason:'):\n",
    "                details['Reason'] = ': '.join(line.split(': ')[1:])\n",
    "        return details\n",
    "\n",
    "    # Extract details for all tasks\n",
    "    curriculum_info = [extract_task_details(section) for section in task_sections]\n",
    "    curriculum_length = len(curriculum_info)\n",
    "    \n",
    "    return curriculum_info, curriculum_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_agent(logger_path, curriculum_info, curriculum_length):\n",
    "    task_list = []\n",
    "    best_agent_list = []\n",
    "    for idx in range(curriculum_length):\n",
    "        curriculum_name = curriculum_info[idx]['Name']\n",
    "        task_list.append(curriculum_name)\n",
    "        try:\n",
    "            decision = file_to_string(logger_path + curriculum_name + '.md')\n",
    "            decision = decision.split('\\n')[0]\n",
    "            numbers = re.findall(r'\\d+', decision)\n",
    "        except:\n",
    "            numbers = [0]\n",
    "        if numbers:\n",
    "            best_agent_list.append(int(numbers[0]))\n",
    "        else:\n",
    "            print(f\"No number found in the decision {idx}\")\n",
    "            best_agent_list.append(0)\n",
    "            \n",
    "    return task_list, best_agent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(logger_path, curriculum_exp_num, her_exp_num, sac_exp_num, scratch_exp_num):\n",
    "    window_size = 10\n",
    "    # Load the training logs for curriculum experiment\n",
    "    curriculum_success_list = []\n",
    "    for i in range(curriculum_exp_num):\n",
    "        print(\"Loading curriculum experiment: \", i)\n",
    "        curriculum_logger_path = logger_path + \"curriculum_\" + str(i) + \"/\"\n",
    "        # if i == 3 or i == 4:\n",
    "        #     print(\"Warning: This is using previous results\")\n",
    "        #     curriculum_logger_path = logger_path + \"curriculum_\" + str(i) + \"_previous/\" \n",
    "        curriculum_info, curriculum_length = extract_curriculum(curriculum_logger_path)\n",
    "        task_list, best_sample_idx = extract_best_agent(curriculum_logger_path, curriculum_info, curriculum_length)\n",
    "\n",
    "        reward_main_list, reward_task_list, success_list, task_length_list = [], [], [], []\n",
    "        for idx, task in enumerate(task_list):\n",
    "            path = curriculum_logger_path + task + f\"/sample_{best_sample_idx[idx]}\"\n",
    "            \n",
    "            reward_df, success = load_curriculum_training_log(path)\n",
    "\n",
    "            # print(\"Reward arguments in Task: \", task)\n",
    "            # for key in reward_df.keys():\n",
    "            #     print(key)\n",
    "\n",
    "            reward_main_list.append(reward_df[\"main\"])\n",
    "            reward_task_list.append(reward_df[\"task\"])\n",
    "            success_list.append(success)\n",
    "            task_length_list.append(len(reward_df[\"main\"]))\n",
    "\n",
    "        reward_main = np.concatenate(reward_main_list, axis=0)\n",
    "        reward_task = np.concatenate(reward_task_list, axis=0)\n",
    "        success_list = np.concatenate(success_list, axis=0)\n",
    "\n",
    "        # compute the moving average of success rate\n",
    "        success_moving_avg = pd.Series(success_list).rolling(window_size).mean().values\n",
    "        curriculum_success_list.append(success_moving_avg)\n",
    "\n",
    "    # Load the training logs for HER experiment\n",
    "    her_success_list = []\n",
    "    for i in range(her_exp_num):\n",
    "        print(\"Loading HER experiment: \", i)\n",
    "        her_logger_path = logger_path + \"her_\" + str(i) + \"/\"\n",
    "        reward_main, success = load_training_log(her_logger_path)\n",
    "        \n",
    "        # compute the moving average of success rate\n",
    "        success_moving_avg = pd.Series(success).rolling(window_size).mean().values\n",
    "        her_success_list.append(success_moving_avg)\n",
    "\n",
    "    # Load the training logs for SAC experiment\n",
    "    sac_success_list = []\n",
    "    for i in range(sac_exp_num):\n",
    "        print(\"Loading SAC experiment: \", i)\n",
    "        sac_logger_path = logger_path + \"sac_\" + str(i) + \"/\"\n",
    "        reward_main, success = load_training_log(sac_logger_path)\n",
    "        \n",
    "        # compute the moving average of success rate\n",
    "        success_moving_avg = pd.Series(success).rolling(window_size).mean().values\n",
    "        sac_success_list.append(success_moving_avg)\n",
    "\n",
    "    # Load the training logs for Scratch experiment\n",
    "    scratch_success_list = []\n",
    "    for i in range(scratch_exp_num):\n",
    "        print(\"Loading Scratch experiment: \", i)\n",
    "        scratch_logger_path = logger_path + \"scratch_\" + str(i) + \"/\"\n",
    "        reward_main, success = load_training_log(scratch_logger_path)\n",
    "        \n",
    "        # compute the moving average of success rate\n",
    "        success_moving_avg = pd.Series(success).rolling(window_size).mean().values\n",
    "        scratch_success_list.append(success_moving_avg)\n",
    "\n",
    "    return curriculum_success_list, her_success_list, sac_success_list, scratch_success_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(logger_path, curriculum_success_list, her_success_list, sac_success_list, scratch_success_list):\n",
    "    plt.rcParams[\"font.family\"] = 'serif'\n",
    "    plt.rcParams[\"font.sans-serif\"] = ['Palatino'] + plt.rcParams[\"font.sans-serif\"]\n",
    "    # Plot the success rate in same figure\n",
    "\n",
    "    def extend_length(success_list):\n",
    "        if not success_list:\n",
    "            return np.array([])\n",
    "        b = np.zeros([len(success_list),len(max(success_list,key = lambda x: len(x)))])\n",
    "        for i,j in enumerate(success_list):\n",
    "            b[i][0:len(j)] = j\n",
    "            b[i][len(j):] = j[-1] # np.nan\n",
    "\n",
    "        return b\n",
    "    curriculum_success_list = extend_length(curriculum_success_list) # np.array(curriculum_success_list)\n",
    "    her_success_list = extend_length(her_success_list)\n",
    "    sac_success_list = extend_length(sac_success_list)\n",
    "    scratch_success_list = extend_length(scratch_success_list)\n",
    "\n",
    "    curriculum_success_avg = np.nanmean(curriculum_success_list, axis=0)\n",
    "    her_success_avg = np.nanmean(her_success_list, axis=0)\n",
    "    sac_success_avg = np.nanmean(sac_success_list, axis=0)\n",
    "    scratch_success_avg = np.nanmean(scratch_success_list, axis=0)\n",
    "\n",
    "    curriculum_success_std = np.nanstd(curriculum_success_list, axis=0)\n",
    "    her_success_std = np.nanstd(her_success_list, axis=0)\n",
    "    sac_success_std = np.nanstd(sac_success_list, axis=0)\n",
    "    scratch_success_std = np.nanstd(scratch_success_list, axis=0)\n",
    "\n",
    "    # Find the shortest length\n",
    "    min_length = min(len(curriculum_success_avg), len(her_success_avg), len(sac_success_avg), len(scratch_success_avg))\n",
    "    curriculum_success_avg = curriculum_success_avg[:min_length]\n",
    "    her_success_avg = her_success_avg[:min_length]\n",
    "    sac_success_avg = sac_success_avg[:min_length]\n",
    "    scratch_success_avg = scratch_success_avg[:min_length]\n",
    "    curriculum_success_std = curriculum_success_std[:min_length]\n",
    "    her_success_std = her_success_std[:min_length]\n",
    "    sac_success_std = sac_success_std[:min_length]\n",
    "    scratch_success_std = scratch_success_std[:min_length]\n",
    "\n",
    "    # Assign color to each experiments\n",
    "    curriculum_color = 'orange'\n",
    "    her_color = 'forestgreen'\n",
    "    sac_color = 'deepskyblue'\n",
    "    scratch_color = 'orangered' #'royalblue', 'orangered'\n",
    "\n",
    "    # Plot the success rate\n",
    "    plt.figure()\n",
    "    plt.plot(curriculum_success_avg, label='CurricuLLM (Ours)', color=curriculum_color)\n",
    "    plt.fill_between(np.arange(len(curriculum_success_avg)), curriculum_success_avg - curriculum_success_std, curriculum_success_avg + curriculum_success_std, color=curriculum_color, alpha=0.2)\n",
    "    if her_success_list.size != 0:\n",
    "        plt.plot(her_success_avg, label='HER', color=her_color)\n",
    "        plt.fill_between(np.arange(len(her_success_avg)), her_success_avg - her_success_std, her_success_avg + her_success_std, color=her_color, alpha=0.2)\n",
    "    if sac_success_list.size != 0:\n",
    "        plt.plot(sac_success_avg, label='SAC', color=sac_color)\n",
    "        plt.fill_between(np.arange(len(sac_success_avg)), sac_success_avg - sac_success_std, sac_success_avg + sac_success_std, color=sac_color, alpha=0.2)\n",
    "    if scratch_success_list.size != 0:\n",
    "        plt.plot(scratch_success_avg, label='LLM-zeroshot', color=scratch_color)\n",
    "        plt.fill_between(np.arange(len(scratch_success_avg)), scratch_success_avg - scratch_success_std, scratch_success_avg + scratch_success_std, color=scratch_color, alpha=0.2)\n",
    "    plt.xlabel('Episodes (x1000)', fontsize=14)\n",
    "    plt.ylabel('Success Rate', fontsize=14)\n",
    "    plt.legend(frameon=False, fontsize=14)\n",
    "    plt.title('Success Rate', fontsize=16)\n",
    "    \n",
    "    # Remove the top and right axes\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    # Save the figure to logger path\n",
    "    plt.savefig(logger_path + \"success_rate.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading curriculum experiment:  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m scratch_exp_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m curriculum_success_list, her_success_list, sac_success_list, scratch_success_list \u001b[38;5;241m=\u001b[39m summarize_results(logger_path, curriculum_exp_num, her_exp_num, sac_exp_num, scratch_exp_num)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mplot_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogger_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurriculum_success_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mher_success_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msac_success_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscratch_success_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 31\u001b[0m, in \u001b[0;36mplot_all\u001b[0;34m(logger_path, curriculum_success_list, her_success_list, sac_success_list, scratch_success_list)\u001b[0m\n\u001b[1;32m     28\u001b[0m scratch_success_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanstd(scratch_success_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Find the shortest length\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m min_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(curriculum_success_avg), \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mher_success_avg\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mlen\u001b[39m(sac_success_avg), \u001b[38;5;28mlen\u001b[39m(scratch_success_avg))\n\u001b[1;32m     32\u001b[0m curriculum_success_avg \u001b[38;5;241m=\u001b[39m curriculum_success_avg[:min_length]\n\u001b[1;32m     33\u001b[0m her_success_avg \u001b[38;5;241m=\u001b[39m her_success_avg[:min_length]\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "logger_path = \"logs/FetchSlide/FetchSlide/\"\n",
    "curriculum_exp_num = 1\n",
    "her_exp_num = 1\n",
    "sac_exp_num = 1\n",
    "scratch_exp_num = 1\n",
    "\n",
    "curriculum_success_list, her_success_list, sac_success_list, scratch_success_list = summarize_results(logger_path, curriculum_exp_num, her_exp_num, sac_exp_num, scratch_exp_num)\n",
    "plot_all(logger_path, curriculum_success_list, her_success_list, sac_success_list, scratch_success_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CurricuLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
